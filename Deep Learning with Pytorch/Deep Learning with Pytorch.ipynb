{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "Pytorch is an open source deep learning library developed by the Facebook AI research group, based on Torch.  Essentially, it provides the user with two main utilies :\n",
    " 1. It allows to perform Tensor/vector computation on the GPU with an API similar (but not compatible) to Numpy.\n",
    " 2. It records all computation to be able to backpropagate through them. That is, provided a sequence of operations that starts from a tensor $\\theta$ to define a scalar $g(\\theta)$, it is able to compute $\\nabla_\\theta g(\\theta)$ exactly, with only one function call.\n",
    " \n",
    " ![](images/pytorch.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Steps to create an ML App \n",
    "To create a machine learning app, we must follow these steps : \n",
    "\n",
    "1. Gathering data\n",
    "2. Data pre-processing\n",
    "3. Researching the model that will be best for the type of data\n",
    "4. Training and testing the model\n",
    "5. Evaluation\n",
    "6. Deploy \n",
    "\n",
    "<img src=\"images/steps.PNG\" alt=\"Drawing\" style=\"width: 700px;\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "For regression analysis we can use **The linear regression machine learning algorithm**, but when it comes to classification we use **logistic regression** which models the probabilities for classification problems with two possible outcomes. So it’s an extension of the linear regression model for classification problems (to tackle multi-class classification problems, variants of logistic regression exist e.g., the One-vs-All or One-vs-One approaches)\n",
    "\n",
    "A linear model treats the classes as numbers (0 and 1) and fits the best hyperplane (for a single feature, it is a line) that minimizes the distances between the points and the hyperplane. it gives estimated values below zero and above one which means that there is no meaningful threshold at which you can distinguish one class from the other. So we need to squash the output of the linear equation into a range of [0,1]. To do this, we perform a non-linear transform on output using the sigmoid function :\n",
    "\n",
    "![](./images/sigmoid_fn.png)\n",
    "![](./images/sigmoid.png)\n",
    "\n",
    "In the linear regression model, we have modelled the relationship between outcome and features with a linear equation:\n",
    "\n",
    "![](./images/linear.png)\n",
    "\n",
    "For classification using logistic regression, we wrap the right side of the equation into the logistic function. This forces the output to assume only values between 0 and 1 interpreted as the probability of having an **input** in a **class C** :\n",
    "\n",
    "![](./images/squeezed.png)\n",
    "\n",
    "We can reformulate the equation that only the linear term is on the right side of the formula :\n",
    "\n",
    "![](./images/logodd.png)\n",
    "\n",
    "The term in the log() function is called \"odds\", and wrapped in the logarithm it is called \"log odds\". We can see that the logistic regression model is a linear model for the log odds.\n",
    "\n",
    "For the cost we use a logarithmic loss function to calculate the cost for misclassifying :\n",
    "![](./images/cost.png)\n",
    "\n",
    "The above cost function can be rewritten as below since calculating gradients from the above equation is difficult :\n",
    "![](./images/compact.png)\n",
    "\n",
    "We take partial derivatives of the cost function with respect to each parameter(theta_0, theta_1, …) to obtain the gradients :\n",
    "![](./images/gradient.png)\n",
    "\n",
    "Neural networks are somewhat related to logistic regression. Basically, we can think of logistic regression as a one layer neural network :\n",
    "![](./images/lrnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Neural Networks \n",
    "Now that we understand how each block of a neural networks works, we can easily understand the whole process. \n",
    "\n",
    "<img src=\"images/neural.PNG\" alt=\"Drawing\" style=\"width: 700px;\"/> \n",
    "\n",
    "### FeedForward \n",
    "\n",
    "<img src=\"images/feed.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "### Backpropagation \n",
    "\n",
    "Backpropagation is the central mechanism by which neural networks learn. It is the messenger telling the network whether or not the net made a mistake when it made a prediction.\n",
    "\n",
    "Our goal with backpropagation is to update each of the weights in the network so that they cause the actual output to be closer the target output, thereby minimizing the error for each output neuron and the network as a whole.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors \n",
    "\n",
    "**What is a tensor?**\n",
    "\n",
    "The inputs, outputs, and transformations within neural networks are all represented using tensors, and as a result, neural network programming utilizes tensors heavily.\n",
    "\n",
    "```A tensor is the primary data structure used by neural networks.```\n",
    "\n",
    "Tensors support some additional enhancements which make them unique: Apart from CPU, they can be loaded or the GPU for faster computations. \n",
    "\n",
    "Before we dive in with specific tensor operations, let’s get a quick overview of the landscape by looking at the main operation categories that encompass the operations we’ll cover. We have the following high-level categories of operations:\n",
    "\n",
    "1. Reshaping operations\n",
    "2. Element-wise operations\n",
    "3. Reduction operations\n",
    "4. Access operations\n",
    "\n",
    "The goal of this section is to not only showcase specific tensor operations commonly used, but to also describe the operation landscape. Having knowledge of the types of operations that exist can stay with us longer than just knowing or memorizing individual operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape([1,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape([1,12]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(1, -1).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "On setting .requires_grad = True in pytorch tensors they start forming a backward graph that tracks every operation applied on them to calculate the gradients using something called a dynamic computation graph (DCG).\n",
    "\n",
    "**Autograd:** This class is an engine to calculate derivatives (Jacobian-vector product to be more precise). It records a graph of all the operations performed on a gradient enabled tensor and creates an acyclic graph called the dynamic computational graph. The leaves of this graph are input tensors and the roots are output tensors. Gradients are calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "data: 1.0\n",
      "requires_grad: True\n",
      "grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "\n",
      "y\n",
      "data: 2.0\n",
      "requires_grad: False\n",
      "grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "\n",
      "z\n",
      "data: 2.0\n",
      "requires_grad: True\n",
      "grad: None\n",
      "grad_fn: <MulBackward0 object at 0x7f04548beeb8>\n",
      "is_leaf: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating the graph\n",
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "y = torch.tensor(2.0)\n",
    "z = x * y\n",
    "\n",
    "# Displaying\n",
    "for i, name in zip([x, y, z], \"xyz\"):\n",
    "    print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\\n",
    "grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop PyTorch from tracking the history and forming the backward graph, the code can be wrapped inside with ``` torch.no_grad():``` It will make the code run faster whenever gradient tracking is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Creating the graph\n",
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "z = x ** 3\n",
    "z.backward() #Computes the gradient \n",
    "print(x.grad.data) #Prints '3' which is dz/dx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete example \n",
    "In this example we're going to build an ML APP that detects the type of clothes in an image. We're going to use the standard Fashion MNIST dataset. \n",
    "This dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class FashionMNIST(datasets.MNIST):\n",
    "    \"\"\"`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``Fashion-MNIST/processed/training.pt``\n",
    "            and  ``Fashion-MNIST/processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "    urls = [\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz',\n",
    "    ]\n",
    "    classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to work with the fashion MNIST dataset. \n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "\n",
    "trainset = FashionMNIST('./F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('./F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class DatasetTransformer(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, base_dataset, transform):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.base_dataset[index]\n",
    "        return self.transform(img), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "\n",
    "train_dataset = DatasetTransformer(train_dataset, transforms.ToTensor())\n",
    "valid_dataset = DatasetTransformer(valid_dataset, transforms.ToTensor())\n",
    "test_dataset  = DatasetTransformer(test_dataset , transforms.ToTensor())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rifux/anaconda2/envs/pytorch/lib/python3.6/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.train_labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rifux/anaconda2/envs/pytorch/lib/python3.6/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.train_labels.bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCorhJwqK4ScKiuEnCorhJwrq/wHgLMwkRjax6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f044962e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAKhCAYAAAC7A2LIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEgIBwBA8l+gpgOl504pwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04548bea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "image, label = next(iter(trainloader))\n",
    "\n",
    "plt.imshow(image[0,:].squeeze(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.515413916711487\n",
      "Training loss: 0.3917721413504849\n",
      "Training loss: 0.35961190372832547\n",
      "Training loss: 0.33329101770258407\n",
      "Training loss: 0.3195688672570277\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[1]\n",
    "\n",
    "# TODO: Calculate the class probabilities (softmax) for img\n",
    "ps = torch.exp(model(img))\n",
    "\n",
    "# Plot the image and probabilities\n",
    "view_classify(img, ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation & dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30..  Training Loss: 0.513..  Test Loss: 0.437..  Test Accuracy: 0.842\n",
      "Epoch: 2/30..  Training Loss: 0.388..  Test Loss: 0.409..  Test Accuracy: 0.854\n",
      "Epoch: 3/30..  Training Loss: 0.354..  Test Loss: 0.395..  Test Accuracy: 0.855\n",
      "Epoch: 4/30..  Training Loss: 0.333..  Test Loss: 0.400..  Test Accuracy: 0.856\n",
      "Epoch: 5/30..  Training Loss: 0.315..  Test Loss: 0.384..  Test Accuracy: 0.864\n",
      "Epoch: 6/30..  Training Loss: 0.301..  Test Loss: 0.368..  Test Accuracy: 0.869\n",
      "Epoch: 7/30..  Training Loss: 0.291..  Test Loss: 0.359..  Test Accuracy: 0.875\n",
      "Epoch: 8/30..  Training Loss: 0.282..  Test Loss: 0.378..  Test Accuracy: 0.870\n",
      "Epoch: 9/30..  Training Loss: 0.275..  Test Loss: 0.366..  Test Accuracy: 0.878\n",
      "Epoch: 10/30..  Training Loss: 0.268..  Test Loss: 0.368..  Test Accuracy: 0.876\n",
      "Epoch: 11/30..  Training Loss: 0.260..  Test Loss: 0.379..  Test Accuracy: 0.871\n",
      "Epoch: 12/30..  Training Loss: 0.256..  Test Loss: 0.376..  Test Accuracy: 0.875\n",
      "Epoch: 13/30..  Training Loss: 0.245..  Test Loss: 0.364..  Test Accuracy: 0.880\n",
      "Epoch: 14/30..  Training Loss: 0.241..  Test Loss: 0.366..  Test Accuracy: 0.880\n",
      "Epoch: 15/30..  Training Loss: 0.241..  Test Loss: 0.354..  Test Accuracy: 0.883\n",
      "Epoch: 16/30..  Training Loss: 0.234..  Test Loss: 0.372..  Test Accuracy: 0.876\n",
      "Epoch: 17/30..  Training Loss: 0.227..  Test Loss: 0.372..  Test Accuracy: 0.881\n",
      "Epoch: 18/30..  Training Loss: 0.227..  Test Loss: 0.401..  Test Accuracy: 0.873\n",
      "Epoch: 19/30..  Training Loss: 0.221..  Test Loss: 0.398..  Test Accuracy: 0.883\n",
      "Epoch: 20/30..  Training Loss: 0.215..  Test Loss: 0.377..  Test Accuracy: 0.886\n",
      "Epoch: 21/30..  Training Loss: 0.219..  Test Loss: 0.381..  Test Accuracy: 0.875\n",
      "Epoch: 22/30..  Training Loss: 0.210..  Test Loss: 0.385..  Test Accuracy: 0.877\n",
      "Epoch: 23/30..  Training Loss: 0.203..  Test Loss: 0.404..  Test Accuracy: 0.881\n",
      "Epoch: 24/30..  Training Loss: 0.204..  Test Loss: 0.401..  Test Accuracy: 0.887\n",
      "Epoch: 25/30..  Training Loss: 0.204..  Test Loss: 0.386..  Test Accuracy: 0.885\n",
      "Epoch: 26/30..  Training Loss: 0.198..  Test Loss: 0.410..  Test Accuracy: 0.886\n",
      "Epoch: 27/30..  Training Loss: 0.190..  Test Loss: 0.396..  Test Accuracy: 0.883\n",
      "Epoch: 28/30..  Training Loss: 0.190..  Test Loss: 0.431..  Test Accuracy: 0.882\n",
      "Epoch: 29/30..  Training Loss: 0.189..  Test Loss: 0.412..  Test Accuracy: 0.881\n",
      "Epoch: 30/30..  Training Loss: 0.188..  Test Loss: 0.437..  Test Accuracy: 0.878\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs \n",
    "CNNs are a type of deep layer neural networks, used to learn Filters that when convolved with the image, can be used to extract features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = CNN();\n",
    "#loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.03);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python [conda env:mypython3]",
   "language": "python",
   "name": "conda-env-mypython3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
